# Orthogonal and Orthonormal Sets

## Introduction

While there are many bases which will span a given Vector space, the standard basis is often the easiest and most common to work with. To use any other basis, we first would have to change the coordinates, an often lengthy process. Orthonormal bases are bases similar to the standard basis in that they are simple to work with and have several useful properties.

### Orthonormal Bases

Note: for these examples, the inner product being used is the standard scalar product $x^Ty$, and the norm is the standard $\sqrt{x^Ty}$.

> Defintion: Othogonal set  
> Given an inner product space $V$ and a set of vectors $\{v_1, v_2...v_n\}$, the set
> is considered an **orthogonal set** if they are pair-wise orthogonal. This means every vector
> is orthogonal to every other vector:  
> $v_i \perp v_j$ for each $i, j$ in $n$

~~Example 1

Test 

$$
\left\{
  \left(
  \begin{array}{c}
  1 \\ 1
  \end{array}
  \right),
  \left(
  \begin{array}{c}
  1 \\ -1
  \end{array}
  \right)
\right\}
$$

is an orthogonal set.

$$
  \left(
  \begin{array}{c}
  1 \\ 1
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  1 \\ -1
  \end{array}
  \right) = 
  1 * 1 - 1 * 1 = 0
$$

Also test:

$$
\left\{
  \left(
  \begin{array}{c}
  1 \\ 1 \\ 1
  \end{array}
  \right),
  \left(
  \begin{array}{c}
  2 \\ 1 \\ -3
  \end{array}
  \right),
  \left(
  \begin{array}{c}
  4 \\ -5 \\ 1
  \end{array}
  \right)
\right\}
$$

Here we need to test every pair:

$$
  \left(
  \begin{array}{c}
  1 \\ 1 \\ 1
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  2 \\ 1 \\ -3
  \end{array}
  \right) = 
  2 + 1 - 3 = 0
$$

$$
  \left(
  \begin{array}{c}
  1 \\ 1 \\ 1
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  4 \\ -5 \\ 1
  \end{array}
  \right) = 
  4 - 5 + 1 = 0
$$

$$
  \left(
  \begin{array}{c}
  2 \\ 1 \\ -3
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  4 \\ -5 \\ 1
  \end{array}
  \right) =
  8 - 5 - 3 = 0
$$

~~

> Definition: Orthonormal Sets  
> A set of vectors $\{v_1, v_2 ... v_n\}$ is **orthonormal** if it is an orthogonal set and $||v_i|| = 1$$.
> Alternatively, $\langle v_i, v_j \rangle = \begin{cases}0: i \ne j \\ 1: i = j\end{cases}$.

~~Example 1.1

Given
$$
\left\{
  \left(
  \begin{array}{c}
  1 \\ 1
  \end{array}
  \right),
  \left(
  \begin{array}{c}
  1 \\ -1
  \end{array}
  \right)
\right\}
$$

Is an orthogonal set, "normalize" it to get an orthonormal set.

1. 
$$
\frac{u_1}{||u_1||} = {1 \over \sqrt{1^2 + 1^2}}
  \left(
  \begin{array}{c}
  1 \\ 1
  \end{array}
  \right) = 
  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
  \end{array}
  \right)
$$

2. 
$$
\frac{u_2}{||u_2||} = {1 \over \sqrt{1^2 + (-1)^2}}
  \left(
  \begin{array}{c}
  1 \\ -1
  \end{array}
  \right) = 
  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}
  \end{array}
  \right)
$$

~~

## Properties

1. Linear independence

This applies automatically if a set is orthogonal.

> Theorem:  
> If $\{v_1...v_n\}$ is an orthogonal set, then $v_1 .. v_n$ are independent.

~~Proof 1

To show they are independent, we try to solve:

$$\alpha_1v_1 + \alpha_2v_2...\alpha_nv_n = 0$$

Here, we are going to try to take the inner product of each side with $v_1$:

$$\langle\alpha_1v_1 + \alpha_2v_2...\alpha_nv_n, v_1\rangle = \langle 0 , v_1\rangle = 0$$

Simplifying, all of the elements which are not $v_1$ will can cancel out, as $v_1$ is orthogonal to all other vectors in the set, leaving only:

$$\alpha_1\langle v_1, v_1 \rangle = \alpha_1 ||v_1||^2 = 0$$

Proving that $\alpha_1 = 0$. This can be repeated for all other vectors in the set.

~~

Relatedly, since having independence as well as the same number of vectors as the dimension of a set proves basis, if $dim(V) = n$ and $\{v_1 ... v_n\}$ is orthonormal, then it is a basis of $V$. We call this an **orthonormal basis**. (This also holds true for orthogonal sets, but then it wouldn't be called an *orthonormal* basis.)

2. Coordinates relative to an ortho*normal* basis

> Theorem:  
> With an orthonormal basis $u_1...u_n$, the coordinates of $v$, $(c_1, ...c_n)$ can be computed as:  
> $$c_i = \langle v, u_i \rangle$$

~~Proof 2

Given $\{u_1...u_n\}$ is an orthonormal basis of $V$, and $v \in V$ with coordinates $(c_1, c_2, ... c_n)$ relative to the orthonormal basis, how can we quickly compute $c_i$ (the coordinates)?

Note: this uses the same trick as proving independence, taking the inner product of the left and right hand sides with each vector.

$$\langle v, u_1\rangle = \langle c_1u_1 + c_2u_2 ... c_n u_n, u_1 \rangle$$
$$\langle v, u_1\rangle = c_1\langle u_1, u_1 \rangle + c_2\langle u_2, u_1 \rangle ... c_n\langle  u_n, u_1 \rangle$$
$$\langle v, u_1\rangle = c_1 || u_1 || ^2 = c_1 * 1 = c_1$$

Therefore, $c_i = \langle v, u_i\rangle$, so there is no need for a transformation matrix.

~~

~~Example 2

Find the coordinates of $(3,4)^T$ relative to:

$$
\left\{

  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
  \end{array}
  \right),
  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}
  \end{array}
  \right)

\right\}
$$

Here we can simply multiply each vector:

$$
c_1 = 
  \left(
  \begin{array}{c}
  3 \\ 4
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
  \end{array}
  \right) = \frac{7}{\sqrt{2}}
$$

$$
c_2 = 
  \left(
  \begin{array}{c}
  3 \\ 4
  \end{array}
  \right)^T
  \left(
  \begin{array}{c}
  \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}
  \end{array}
  \right) = \frac{-1}{\sqrt{2}}
$$

And $(3,4)^T$ can be written as:


$$
  \left(
  \begin{array}{c}
  3 \\ 4
  \end{array}
  \right) = 
  \frac{7}{\sqrt{2}}u_1 +
  \frac{-1}{\sqrt{2}}u_2
$$

~~

~~Corrolaries for an orthonormal basis

Notice how many of these properties are very similar to the way that these operations work in the standard basis.

Given $\{u_1...u_n\}$, $v = a_1u_1 + ... a_n u_n$, and $w = b_1u_1 ...b_nu_n$

1. Inner product formula:

$$\langle v, w\rangle = a_1b_1 + a_2 b_2 ... a_n b_n$$

2. Norm formula:

$$\langle v, v \rangle = ||v||^2 = a_1^2 + a_2^2 ... a_n^2$$

~~

3. Projection

With a vector $x$, we want to find it's components relative to a space $S$, where $p \in S$ and $z \perp S$.

> Theorem:  
> Given a subspace of the vector space $V$ called $S$ and an orthonormal basis $\{u_1, u_2 ... u_n\}$ for the space $S$, then the vector projection of $x$ on $S$ can be computed by:  
> $$p = c_1u_1 + c_2u_2 + ... c_nu_n$$  
> Where $c_i = \langle x, u_i$.

~~Proof 3

Because $p$ is in $S$, $p = c_1u1 + ... c_n u_n$, we have:

$$c_i = \langle p, u_i \rangle$$

Since $x = p + z$,

$$c_i = \langle x - z, u_i \rangle$$
$$c_i = \langle x, u_i \rangle - \langle z, u_i \rangle$$

And since $z \perp S$, and $\{u_1...u_n\}$ are a basis of $S$:

$$c_i = \langle x, u_i \rangle - 0$$

~~

In addition, since we have (in $\mathbb{R}^n$):

$$p = c_1u_1 + c_nu_n$$
$$p = (x^Tu_1)u_1 + (x^Tu_n)u_n$$
$$p = (u_1^Tx)u_1 + (u_n^Tx)u_n$$

We can write it in matrix form:

$$
U 
\left[
  \begin{array}
    u_1^Tx \\ ... \\ u_n^Tx
  \end{array}
\right]
=
UU^Tx
$$

Where $U = [u_1, u_2, ... u_n]$

So we have $p = UU^Tx$

~~Example 3

Given $S = xy-plane = \{(x, y, 0)^T\} = span\{(1,0,0)^T, (0,1,0)^T\}$, find the vector projection of $(5,3,4)$ on $S$.

Importantly, the set $\{(1,0,0)^T, (0,1,0)^T\}$ is an orthonormal basis, so we can use the formula:

$$p = c_1e_1 + c_2e_2$$

$$c_1 = \langle x, e_1\langle = x^Te_1 = 5$$
$$c_2 = \langle x, e_2\langle = x^Te_2 = 3$$

We also could have used the formula:

$$p = UU^Tx$$

$$
\left[
  \begin{array}{cc}
  1 & 0 \\
  0 & 1 \\
  0 & 0
  \end{array}  
\right]
\left[
  \begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  \end{array}  
\right]
\left[
  \begin{array}{c}
  5 \\ 3 \\ 4
  \end{array}  
\right]
$$

$$
\left[
  \begin{array}{cc}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 0
  \end{array}
\right]
\left[
  \begin{array}{c}
  5 \\ 3 \\ 4
  \end{array}  
\right]
=
\left[
  \begin{array}{c}
  5 \\ 3 \\ 0
  \end{array}  
\right]
$$

~~

## Application

### Least Square problem

Remember that we want to find a vector $\hat x$ such that it minimizes error $||b - Ax||$ in the system $Ax = b$. If the columns of $A$ form an orthonormal set, then they form an orthonormal basis of $R(A)$. 

Using the formula for finding a projection of $b$ into $Ax$:

$$A\hat x = AA^Tb$$

It looks like $\hat x = A^Tb$, and we can actually prove that for such an $A$, $A^TA = I$, allowing us to prove that $\hat x = A^Tb$.

A matrix whose columns form an orthonormal basis like this is called an orthogonal matrix.

<script src="/source/texme.js"></script>
<link rel="stylesheet" href="/source/theme.css">
