# Matrices

## Matrix Operations

> **Definition:** Matrix  
> If $A$ is an $m\times n$ matrix, then:  
> $A = \begin{bmatrix}a_11 && ... && a_1n \\ ... \\ a_m1 && ... && a_mn\end{bmatrix}$  
> Where $a_ij$ is the entry on the $i^{th}$ row and
> $j^{th}$ column.  
> They are normally denoted as $\textbf{A}$, or simply $A$.

> **Definition:** Vector  
> Vectors can be thought of as a $\begin{aligned}\end{aligned}1 \times n$ or $m \times 1$ matrix,
> a row vector and column vector, respectively.  
> They are normally denoted as $\vec a$ or $\textbf{a}$

### Equality

> **Definition:** Two $m\times n$ matrices $\textbf{A}$ and $\textbf{B}$
> are equal if all of their components (in corresponding locations)
> are equal.

### Scalar Multiplication

Given:

$$b\textbf{A}$$

The resulting matrix is given by:

$$ba_{ij}$$

For every $i,j$.

$$
\textbf{A} = 
\left[
\begin{array}{cc}
1 & 2 \\ 3 & 4
\end{array}
\right]
$$

$$
b\textbf{A} = 
\left[
\begin{array}{cc}
b1 & b2 \\ b3 & b4
\end{array}
\right]
$$

### Addition/Subtraction

Given:

$$\textbf{A} \pm \textbf{B}$$

The final matrix is given by:

$$a_{ij} \pm b_ij$$

Where $a_{ij}$ is each element in $\textbf{A}$, and the same for $b_{ij}$

### Transpose

Given $\textbf{B}$ is the transpose of $\textbf{A}$:

$$b_{ij} = a_{ji}$$

Which swaps the columns with the rows.

This is normally denoted as $\textbf{A}^T$.

$$
\left[
\begin{array}{cc}
1 & 2 \\ 3 & 4 \\ 5 & 6
\end{array}
\right]^T = 
\left[
\begin{array}{ccc}
1 & 3 & 5 \\ 2 & 4 & 6
\end{array}
\right]
$$

> **Definition:** $\textbf{A}$ is **symmetric** if $\textbf{A} = \textbf{A}^T$

### Matrix Multiplication

If $\textbf{A}$ is a $m \times n$ matrix, and $\textbf{B}$ must be a $n \times r$ matrix. Note that the second dimension of the first matrix and the first of the second are the same.

$$\textbf{C} = \textbf{AB}$$

$$c_{ij} = a_{i1}b{1j} + a_{i2}b_{2j} ... a_{in}b_{nj}$$
$$c_{ij} = \sum\limits_{k=1}^n a_{ik}b_{kj}$$

~~Example 1

$$
\textbf{A} = 
\left[
\begin{array}{cc}
3 & -2 \\ 2 & 4 \\ 1 & -3
\end{array}
\right]
$$
$$
\textbf{B} = 
\left[
\begin{array}{cc}
-2 & 1 \\ 4 & 1
\end{array}
\right]
$$

These can be multiplied, as they have the correct corresponding dimension. So, we can multiply, going row-by-row in the first matrix and column-by-column in the second:

$$
\textbf{AB} = 
\left[
\begin{array}{cc}
3(-2)+(-2)4 & 3(1) + (-2)1 \\ 
2(-2) + (4)4 & 2(1) + 4(1) \\ 
1(-2) + -3(4) & 1(1) + (-3)1
\end{array}
\right]
$$
$$
= 
\left[
\begin{array}{cc}
-14 & 1 \\ 12 & 6 \\ -14 & -2
\end{array}
\right]
$$

~~

Note that often $\textbf{AB} \ne \textbf{BA}$. This is different from normal algebraic rules.

The important feature of matrix multiplication is that it can represent linear systems:

$$\textbf{A}\vec x = \vec b$$

Using this notation represents the corresponding system of equations:

$$
\left[
\begin{array}{ccc}
a_11 & ... & a_1n \\
 &... 
\\ a_m1 & ... & a_mn
\end{array}
\right]
\left[
\begin{array}{c}
x_1 \\ ... \\ x_n
\end{array}
\right]
=
\left[
\begin{array}{c}
b_1 \\ ... \\ b_m
\end{array}
\right]
$$

Is the same as (doing the multiplication):

$$
\begin{cases}
a_11x_1 + ... + a_1nx_n = b_1 \\
... \\
a_m1x_1 + ... + a_mnx_n = b_m \\
\end{cases}
$$

### Power Notation

$$\textbf{A}^2 = \textbf{AA}$$
$$\textbf{A}^k = \textbf{AA}...\textbf{A},\ k\ times.$$

Note that powers can only be used on $n \times n$ matrixes, as otherwise the matrix multiplication would not be possible.

### Identity

> Definition: Identity matrix  
> The identity matrix is defined as the matrix which results in no change after multiplication.
> $\textbf{AI} = \textbf{IA} = \textbf{A}$
> Where $\textbf{I}$ is the identity matrix.
> It has all zero elements except a the diagonal down the center.

$$
\textbf{I}_{2\times 2} = 
\left[
\begin{array}{cc}
1 & 0 \\ 0 & 1
\end{array}
\right]
$$
$$
\textbf{I}_{3\times 3} = 
\left[
\begin{array}{ccc}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
\end{array}
\right]
$$

### Zero Matrix

$$
0 = \textbf{0} = 
\left[
\begin{array}{ccc}
0 & ... & 0 \\ &... \\ 0 & ... & 0
\end{array}
\right]
$$

### Inverse

> Definition: Inverse
> An $n \times n$ matrix $\textbf{A}$ is **non-singular** or **invertible** if there is a matrix $\textbf{B}$ where
> $\textbf{AB} = \textbf{BA} = \textbf{I}$
> ($\textbf{B}$ is the multiplicative inverse of $\textbf{A}$)
> denoted $\textbf{A}^{-1}$

Note that the inverse is unique.

~~Proof

Suppose $\textbf{B}$ and $\textbf{C}$ are inverses of $\textbf{A}$. We need to show that $\textbf{B} = \textbf{C}$.

$$\textbf{AB} = \textbf{I}$$
$$\textbf{CAB} = \textbf{CI}$$
$$\textbf{IB} = \textbf{CI}$$
$$\textbf{B}=\textbf{C}$$

~~

> Definition: Singular
> $\textbf{A}$ is **singular** if it does not have an inverse.

~~Example 2

$$
\left[
\begin{array}{cc}
2 & -1 \\
3 & 1
\end{array}
\right]
$$
$$
\left[
\begin{array}{cc}
\frac{1}{5} & \frac{1}{5} \\
-\frac{3}{5} & \frac{2}{5}
\end{array}
\right]
$$

Verify that these are inverses:

$$
\left[
\begin{array}{cc}
2 & -1 \\
3 & 1
\end{array}
\right]
\left[
\begin{array}{cc}
\frac{1}{5} & \frac{1}{5} \\
-\frac{3}{5} & \frac{2}{5}
\end{array}
\right]
=
\left[
\begin{array}{cc}
1 & 0 \\
0 & 1
\end{array}
\right]
$$

~~

The main use for inverses is to manipulate equations with matrices:

Given a system $\textbf{A}\vec x = \vec b$, If we can find the inverse of $\textbf{A}$, we can solve.

$$\textbf{A}^{-1}\textbf{A}\vec x = \textbf{A}^{-1}b$$
$$\vec x = \textbf{A}^{-1}\vec b$$

Which gives a constant solution for each $x_n$

To find the inverse of a matrix, set up the augmented matrix:

$$
\left[
\begin{array}{c|c}
\textbf{A} & \textbf{I}
\end{array}
\right]
$$

Then transform to reduced row-echelon form to get:
$$
\left[
\begin{array}{c|c}
\textbf{I} & \textbf{A}^{-1}
\end{array}
\right]
$$

~~Example 3
$$
\textbf{A} =
\left[
\begin{array}{cc}
1 & 3 \\
2 & 4
\end{array}
\right]
$$

Find $\textbf{A}^{-1}$ and solve:

$$
\begin{cases}
x + 3y = 1 \\
2x + 4y = 2
\end{cases}
$$

So, we first set up the augmented matrix:
$$
\left[
\begin{array}{c|c}
\textbf{A} & \textbf{I}
\end{array}
\right]
$$
$$
\left[
\begin{array}{cc|cc}
1 & 3 & 1 & 0 \\
2 & 4 & 0 & 1
\end{array}
\right]
$$
$$
\left[
\begin{array}{cc|cc}
1 & 3 & 1 & 0 \\
0 & -2 & -2 & 1
\end{array}
\right]
$$
$$
\left[
\begin{array}{cc|cc}
1 & 3 & 1 & 0 \\
0 & 1 & 1 & -\frac{1}{2}
\end{array}
\right]
$$
$$
\left[
\begin{array}{cc|cc}
1 & 0 & -2 & \frac{3}{2} \\
0 & 1 & 1 & -\frac{1}{2}
\end{array}
\right]
$$
$$
\left[
\begin{array}{c|c}
\textbf{I} & \textbf{A}^{-1}
\end{array}
\right]
$$

So we have:

$$
\textbf{A}^{-1} =
\left[
\begin{array}{cc}
-2 & \frac{3}{2} \\
1 & -\frac{1}{2}
\end{array}
\right]
$$

So we now try to solve the system:

$$\textbf{A}\vec x = \vec b$$
$$\textbf{A}\textbf{A}^{-1}\vec x = \textbf{A}^{-1}\vec b$$
$$\vec x = \textbf{A}^{-1}\vec b$$
$$
\left[
\begin{array}{c}
x \\ y
\end{array}
\right]
= 
\left[
\begin{array}{cc}
-2 & \frac{3}{2} \\
1 & -\frac{1}{2}
\end{array}
\right]
\left[
\begin{array}{c}
1 \\ 2
\end{array}
\right]
$$
$$
\left[
\begin{array}{c}
x \\ y
\end{array}
\right]
= 
\left[
\begin{array}{c}
1 \\ 0
\end{array}
\right]
$$

So the solution is $(1,0)$.

~~

### Determinant

Recall that an $n \times n$ matrix is **non-singular** if it has an inverse (and **singular** if it does not). This means that the inverse can be used to solve the corresponding linear system:

$$\textbf{A}x = b$$
$$x = \textbf{A}^{-1}b$$

> Definition: Determinant  
> The determinant ($det(\textbf{A})\ or\ |\textbf{A}|$) tells us if $\textbf{A}$ is singular.
> - $det(\textbf{A}) = 0$ iff $\textbf{A}$ is singular.
> - $det(\textbf{A}) \ne 0$ iff $\textbf{A}$ is non-singular.

Note, the notation $|\textbf{A}|$ is more commonly used when writing the matrix out:

$$
det(\textbf{A}) =

\left|
\begin{array}{ccc}
a_11 & ... & a_1n \\
 &... 
\\ a_m1 & ... & a_mn
\end{array}
\right|
$$

The determinant of a matrix is computed differently for each size of matrix:

1. $1\times 1$: 
$$|\textbf{A}| = |a| = a$$
2. $2\times 2$:
$$
|\textbf{A}| = 
\left|
\begin{array}{cc}
a & b \\ c & d
\end{array}
\right| = ad-bc
$$
3. General $n\times n$ matrix

The technique here is **cofactor expansion**, 

> Definition: Cofactor  
> If $\textbf{A} = a_{ij}$ for $n\times n$, for each entry $a_{ij}$, let M_{ij} be the $(n - 1) \times (n - 1)$ matrix obtained by removing the $i^{th}$ row and $j^{th}$ column.
> 
> $det(\textbf{M}_{ij})$ is called the minor determinant of $a_{ij}$. 
> The **cofactor** of $a_{ij}$ is $\textbf{A}_{ij} = (-1)^{i +j} det(\textbf{M}_{ij})$

> Defintion: Cofactor expansion  
> Along any $i^{th}$ row, we can perform cof exp to compute the determinant:
> $\textbf{A} = \left[\begin{array}{ccc}&...\\a_i1 & ... & a_in \\ & ...\end{array}\right]$  
> $det(\textbf{A}) = a_{i1}\textbf{A}_{i1} + a_{i1}\textbf{A}_{i2} + ... + a_{in}\textbf{A}_{in}$
> 
> This can be done similarly for any column, multiplying the cofactor of each value in the row with that value.

Note that this computation only needs to be done for one row or column, and that choosing a good row or column can decrease the umber of computations necessary.

~~Example 4

$$
\textbf{A} = 
\left[
\begin{array}{ccc}
1 & 2 & 3 \\ 
3 & 0 & -1 \\
4 & -2 & 1
\end{array}
\right]
$$

First, let's try with the first row.

$$
\left|
\begin{array}{ccc}
1 & 2 & 3 \\ 
3 & 0 & -1 \\
4 & -2 & 1
\end{array}
\right| = 
1 (-1)^{1+1}
\left|
\begin{array}{cc}
0 & -1 \\
-2 & 1
\end{array}
\right| +
2(-1)^{1+2}
\left|
\begin{array}{cc}
3 & -1 \\
4 & 1
\end{array}
\right| +
3(-1)^{1+3}
\left|
\begin{array}{cc}
3 & 0 \\
4 & -2
\end{array}
\right|
$$
$$=1(0-2)-2(3+4)+3(-6-0)=-34$$

Now, notice that the second row has a zero. This means that we will not have to compute the middle value! Let's try that instead.

$$
\left|
\begin{array}{ccc}
1 & 2 & 3 \\ 
3 & 0 & -1 \\
4 & -2 & 1
\end{array}
\right| = 
3 (-1)^{2+1}
\left|
\begin{array}{cc}
2 & 3 \\
-2 & 1
\end{array}
\right| +
0 +
-1(-1)^{2+3}
\left|
\begin{array}{cc}
1 & 2 \\
4 & -2
\end{array}
\right|
$$
$$=-3(2+6)+0+1(-2-8)=-34$$

The same could be done with the middle column.

~~

This can quickly get out of hand with a $4\times 4$ or larger matrix. Then picking the right row or column is essential:

~~Example 5

$$
\textbf{A} = 
\left[
\begin{array}{cccc}
1 & 2 & 3 & 4 \\ 
2 & -1 & 0 & 3 \\
-4 & 1 & 0 & 1 \\
5 & 0 & 0 & 0
\end{array}
\right]
$$

First taking the last row:

$$
\left|
\begin{array}{cccc}
1 & 2 & 3 & 4 \\ 
2 & -1 & 0 & 3 \\
-4 & 1 & 0 & 1 \\
5 & 0 & 0 & 0
\end{array}
\right| =
5 (-1)^{4+1}
\left|
\begin{array}{ccc}
2 & 3 & 4 \\ 
-1 & 0 & 3 \\
1 & 0 & 1 \\
\end{array}
\right| + 0 + 0 + 0
$$

Then the middle column:

$$
\left|
\begin{array}{ccc}
2 & 3 & 4 \\ 
-1 & 0 & 3 \\
1 & 0 & 1 \\
\end{array}
\right| =
3 (-1)^{1+2}
\left|
\begin{array}{cc}
-1 & 3 \\
1 & 1 \\
\end{array}
\right|
 + 0 + 0
$$

So the determinant is:

$$-5(-3(-1-3)) = -60$$

~~

If the determinant exists, the inverse can be found with the following formula:

$$\textbf{A}^{-1} = {1\over |\textbf{A}|}\textbf{A}$$

#### Easy Cases

There are some cases in which it is easy to take the determinant:

1. If there is a row or column of all zeros, the determinant is $0$. (This is easily proved by expanding with that row/column, getting $0$)
2. If the matrix is triangular (all entries above or all entries below are zero), then the determinant is the product of diagonal entries. (This is the same as expanding on the rows with the most zeroes)

#### Elementary operations on determinant

1. Swapping two rows flips the sign of the determinant.
$$
\left|
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right| = -1
\left|
\begin{array}{cc}
c & d \\
a & b \\
\end{array}
\right|
$$
2. Multiplying a row my a scalar increases the determinant by that scalar (multiplying)
$$
\left|
\begin{array}{cc}
a & b \\
\alpha c & \alpha d \\
\end{array}
\right| = \alpha
\left|
\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right|
$$
3. Adding or subtracting multiples of rows from other rows does not change the determinant.

This last rule is useful for making the determinant easier to compute.

~~Example of rule 3

$$
\left|
\begin{array}{cc}
1 & 1 & 3 \\
2 & 0 & 1 \\
-1 & -1 & 1 \\
\end{array}
\right|
$$

First, let's use rule 3 to simplify the matrix:

$$
\left|
\begin{array}{cc}
1 & 1 & 3 \\
2 & 0 & 1 \\
0 & 0 & 4 \\
\end{array}
\right|
$$

Then compute by the third row:

$$
\left|
\begin{array}{cc}
1 & 1 & 3 \\
2 & 0 & 1 \\
0 & 0 & 4 \\
\end{array}
\right| = 
4(-1)^{1+3}(0-2) = -8
$$

~~

## Algebraic Rules for Matrices

Again, remember that the order of matrix operation matters:

$$\textbf{AB} \ne \textbf{BA}$$

### Rules for addition or multiplication:

Note: bold face is not used for (my) convience.

1. $$A + B = B + A$$
2. $$(A+B) + C = A + (B + C)$$
3. $$(AB)C = A(BC)$$
4. $$A(B+C) = AB+AC$$
5. $$(A+B)C = AC+BC$$
6. $$(\alpha\beta)A = \alpha(\beta A)$$
7. $$\alpha( AB )= (\alpha A)B = A(\alpha B)$$
8. $$(\alpha + \beta)A = \alpha A + \beta B$$
9. $$\alpha(A + B) = \alpha A + \alpha B$$

### Rules for transpose:

1. $$(A^T)^T = A$$
2. $$(\alpha A)^T = \alpha (A^T)$$
3. $$(A \pm B)^T = A^T \pm B^T$$
4. $$(AB)^T = B^TA^T$$
5. $$(ABC)^T = C^TB^TA^T\ (etc.)$$

Note in the last two rules, the order of multiplication reverses.

~~Example 6

Remember that $\textbf{A}$ is symmetric if $\textbf{A} = \textbf{A}^T$

Given $\textbf{A}$ and $\textbf{B}$ are symmetric. Are the following symmetric?

1. $\textbf{A} + \textbf{B}$

To check, we need $(\textbf{A} + \textbf{B})^T$.

$$(\textbf{A} + \textbf{B})^T = \textbf{A}^T + \textbf{B}^T = \textbf{A} + \textbf{B}$$

So this is symmetric.

2. $\textbf{A}\textbf{B}$

$$(\textbf{A}\textbf{B})^T = \textbf{B}^T\textbf{A}^T = \textbf{B}\textbf{A}$$

This is not symmetric

~~

### Rules for inverse:

If $A$ and $B$ are non-singular and $AB$ (and $BA$) is non-singular, 

1. $$(AB)^{-1} = B^{-1}A^{-1}$$
2. $$(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$$
3. $$(A^T)^{-1} = (A^{-1})^T$$

~~Example 7

Suppose $\textbf{A}^2 = 0$
Show that $I - A$ and $A - I$ are non-singular and inverses of eachother.

For this to be true,

$$(I -A)(I - A) = I$$
$$I^2 -AI - IA - A^2 = I$$
$$I - A - A = I$$
$$I = I$$

~~

### Rules for determinant:

1. $det(A)=0$ iff $A$ is singular
2. $det(A)\ne0$ iff $A$ is non-singular
3. $det(AB) = det(A)det(B)$ ***commonly used***
4. $det(A+B)\ne det(A)+det(B)$
5. $det(I) = 1$
6. $det(A^{-1}) = \frac{1}{det(A)}$

~~Proof 2
$$det(A^{-1}) = \frac{1}{det(A)}$$
$$det(A^{-1})det(A) = 1$$
$$det(A^{-1}A) = 1$$
$$det(I) = 1$$
~~

7. $det(\alpha A) = \alpha^n det(A)$

~~Proof 3
When multiplying a matrix by a scalar, we multiply every value by that scalar, which is equivalent to multiplying all rows by the scalar (similar to the second basic operation), which multiplies the determinant by that scalar each time it happens. Therefore, the determinant is multiplied by the scalar as many times as there are rows in the matrix.
~~

8. $det(A^T) = det(A)$

~~Example 8

Prove that $AB$ is singular if either $A$ or $B$ is singular.

$AB$ is singular iff $det(AB)$ is $0$.

$$det(AB) = 0$$
$$det(A)det(B) = 0$$

So either $det(A)$ is 0, or $det(B)$ is 0, and the determinant of their product will be zero. Therefore, if either of the matrices is singular, they both will be.

~~

~~Example 9

Recall that $A$ is non-singular if there is a $B$ where $AB = BA = I$

Prove that just proving one order of multiplication $AB = I$ OR $BA=I$ is enough to prove both:

$$det(AB) = det(I)$$
$$det(A)det(B) = 1$$
$$det(A) \ne 0$$
$$det(B) \ne 0$$

Therefore, both matrices will have an identity.

~~

<script src="/source/texme.js"></script>
<link rel="stylesheet" href="/source/theme.css">
