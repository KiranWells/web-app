
# Multiple RVs

It is often necessary to deal with multiple random variables at the same time.

## Joint CDF

The most universal measure of an RV is the CDF, as both discrete and continuous RVs have them. We can create a joint CDF for two variables as:

$$
F(x,y) = Pr(X \le x, Y\le y)
$$

Where $A,B$ is the same as saying $A\cap B$. This CDF can be split into the two components by taking the probability of one and making the other go to 100% probability:

$$F_X(x) = F(x, \infty) = Pr(X \le x, Y \le \infty)$$
$$F_Y(y) = F(\infty, y) = Pr(X \le \infty, Y \le y)$$

This is similar to the Total Probability Theorem, where we can split one probability into the combination of several disjoint sections of it. Here, we are adding up all of the probabilities where $X \le x$ for all values of $y$, and therefore $Y \le \infty$.

These separate PDFs are called the *marginal* CDFs.

### Properties

Finding an interval probability $[(x_1, y_1), (x_2, y_2)]$ can be solved as:

$$
Pr(x_1 \lt X \lt x_2, y_1 \lt Y \lt y_2) = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1)
$$

This is an application of the inclusion-exclusion principle.

## Joint PMF

A point PMF is defined similarly:

$$
p(x,y) = \Pr(X = x, Y = y)
$$

Because this is just a probability of the intersection, *if the two events are independent* we can write the probability as:

$$p_{XY}(x,y) = \Pr(X = x \cap Y = y) = \Pr(X = x)\Pr(Y = y)$$

If the probabilities are not independent, we can still use total probability theorem to simplify the problem, if it applies.

### Multiple Variables

If you have many random variables, you can always simplify to:

$$
\begin{align*}
p(x_1, x_2, x_3 \cdots) &=  \\
  & p(x_2, x_3 \cdots | x_1)p(x_1) + \\
  & p(x_1, x_3 \cdots | x_2)p(x_2) + \\
  & \vdots
\end{align*}
$$

Which can be simplified even further to:

$$
\begin{align*}
p(x_1, x_2, x_3 \cdots) &=  \\
  & p(x_3, x_4 \cdots | x_2, x_1)p(x_2|x_1)p(x_1) + \\
  & p(x_1, x_4 \cdots | x_3, x_2)p(x_3|x_2)p(x_2) + \\
  & \vdots
\end{align*}
$$

And so on. This is difficult to calculate because there will be nearly $2^n$ different probabilities that you need to specify to determine it. This can be approximated by a **Markov Chain Assumption**. This assumes that when looking at a conditional probability at a certain state that you can ignore previous information. In other words, the next step is only dependent on this step, not all of the previous steps. This simplifies the number of probabilities you need to calculate to a linear increase: $\approx 2n$.

See [this link](https://www.geeksforgeeks.org/markov-chain/) for more information.

## Multinomial Random Variables

This is an extension of a binomial random variable, where instead of just a success or failure, there are multiple different results. For example, rolling a die $n$ times and counting all of the times each number comes up. This will create several random variables, $X_1, X_2, \cdots X_6$. The PMF could be described as:

$$
p_{X_1, X_2, \cdots }(x_1,x_2,\cdots ) = \binom{n}{x_1,x_2,\cdots }p_1^{x_1}p_2^{x_2}\cdots 
$$

For this to be valid, the requirement is that the outcomes must be independent, and sum to 1:

$$\sum p_i = 1$$

The marginal PMFs can be found as:

$$p_{X_6}(x_6) = \binom{n}{x_6}p_6^{x_6}(1-p)^{n-x_6}$$

## Joint PDF

We can say $X$ and $Y$ are **jointly continuous** if we can define a joint PDF for them $f(x,y)$ where

$$
Pr((X,Y)\in B) = \int_B f(a,b)da db
$$

~~Example: Circular probability

A circular area has a uniformly distributed probability of an object being at that location. The joint probability of $x$ and $y$ is given as:

$$
f_{XY} = \begin{cases}
c & \sqrt{x^2 + y^2} \le R \\
0 & \textrm{otherwise}
\end{cases}
$$

a) Find the constant $c$ for this to be a valid PDF.

The normalization axiom requires the sum of all the probabilities is 1, so we can take the integral over the entire area:

$$
\int\limits_0^R \int\limits_0^{2\pi} c r d\theta dr
$$

$$= \pi R^2 c \to c = \frac{1}{\pi R^2}$$

b) find the marginal distributions $f_X$ and $f_Y$.

Because the distribution is symmetric, $f_X$ will equal $f_Y$, one is just rotated. So, we only need to find $f_X$. This can be found using the joint PDF by summing all possible values of $y$:

$$f_X(x) = \int\limits_{Y_r} f_{XY}(x,y) $$

This means we need to integrate over the $y$ range at $x$, which is $\sqrt{R^2 - x^2}$:

$$
f_X = \int\limits_{-\sqrt{R^2 - x^2}}^{\sqrt{R^2 - x^2}} \frac{1}{\pi R^2}
$$

$$= \frac{2\sqrt{R^2 - x^2} }{ \pi R^2} $$

~~

## Conditional Distribution Functions

The conditional PMF is defined as:

$$
p_{Y|X}(y|x) = \Pr(Y=y|X=x)
$$

which can be simplified using Bayes' Rule to:

$$\frac{p_{X,Y}(x,y)}{p_X(x)}$$

For conditional distribution functions:

$$
f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
$$

~~Example: Circle revisited

From the previous example, the conditional probability $f_{X|Y}$ can be found as:

$$
\frac{f_{X,Y}(x,y)}{f_Y(y)} =
\frac{\frac{1}{\pi R^2}}{\frac{2\sqrt{R^2 - y^2} }{ \pi R^2}}
= \frac{1}{2\sqrt{R^2-y^2}}
$$

This can be checked by evaluating the normalization axiom.

~~

## Independence

Two random variables are independent if:

$$\Pr(X\in A, Y\in B) = \Pr(X \in A)\Pr(Y \in B)$$

For any set $B$. In other words, $X$ and $Y$ are independent if all of the events inside them are independent. In terms of the joint CDF:

$$F(x,y) = F_X(x)F_Y(y) \quad \forall (x,y) \in \mathbb{R}^2$$

An therefore:

$$f_{XY}(x,y) = f_X(x)f_Y(y) \quad \forall (x,y) \in \mathbb{R}^2$$

For discrete random variables, $X$ and $Y$ are independent if and only if

$$p_{XY}(x,y) = p_X(x)p_Y(y) \quad \forall (x,y) \in \mathbb{R}^2$$

This can also be extended to more than two random variables. If $X_1$, $X_2$, and $X_3$ are independent, then

$$f(x_1, x_2, x_3) = f(x_1)f(x_2)f(x_3)$$

~~Example

Are $X$ and $Y$ independent if their joint PDF is given as:

$$
f(x,y) = \begin{cases}
e^{-(x+y)} & x\gt 0, y\gt 0 \\
0 & \textrm{otherwise}
\end{cases}
$$

To solve this, we need the marginal PDFs.

$$
f(x) = \int_0^{\infty} e^{-(x+y)}dy = e^{-x}\int e^{-y}dy
$$

$$= e^{-x}\left( -e^{-\infty} - (-e^{0}) \right) = e^{-x}$$

The same process shows $f(y) = e^{-y}$. Now, we evaluate:

$$f(x,y) \stackrel{?}{=} f(x)f(y)$$
$$e^{-(x+y)} \stackrel{?}{=} e^{-x}e^{-y}$$

This is true, so they are independent.

b) Find $\Pr(Y \le X)$

To do this, we can find the integral of the region where $y\le x$:

$$\int_0^\infty \int_y^\infty e^{-(y+z)}dxdy$$

$$\int_0^\infty e^{-y} \left( \int_y^\infty e^{-x}dx\right)dy  = \int_0^\infty e^{-2y} = \frac{1}{2}$$

~~

## Functions of Random Variables

Given $X$ and $Y$, and a set $Z = g(X,Y)$, we can determine the PDF or PMF of $Z$. If both are discrete:

$$p_Z(z) = \sum\limits_{(x,y):g(x,y) = z} p(x,y)$$

When both are continuous:

$$F_Z(z) = \Pr(Z \le z) = \int\limits_{(x,y):g(x,y) \le z} f(x,y)$$

~~Example: Circle continued

Let $D$ be the distance from the origin. Find the PDF of $D$.

We know $D$ is:

$$D = \sqrt{X^2 + Y^2}$$

we can calculate the CDF of $D$ as:

$$
F_D(d) = \Pr(D\le d) = \int_D \frac{1}{\pi R^2} dx dy = 
\begin{cases}
  0 & r \lt 0 \\
  \frac{r^2}{R^2} & 0\le r \le R \\
  1 & r \gt R 
\end{cases}
$$

The PDF can then be found by taking the derivative of $F$:

$$
f_D(d) = \frac{dF_D}{dr} = 
\begin{cases}
  0 & r \lt 0, r\gt R \\
  \frac{2r}{R^2} & 0\le r \le R  
\end{cases}
$$

~~

### Sum of two RVs

Given $Z = X+Y$ and $X$ and $Y$ are independent, find $F_Z(z)$.

This is a simple application of the definition:

$$F_Z(z) = \Pr(X+Y \le z) = \iint_{x+y\le z} f_{XY}(x,y)$$

Which will be:

$$
\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^{z-y} f_X(x)f_Y(y)dxdy \\
=\int_{-\infty}^\infty f_Y(y)F_x(z-y)dy
\end{align*}
$$

and the PDF will be:

$$
f_Z(z) = \frac{dF_Z}{dz} = \int_{-\infty}^\infty f_Y(y)f_X(z-y)dy
$$

Which is the convolution of two functions.

~~Example: Square
For example, take the PDF:

$$
f_{XY}(x,y) = \begin{cases} 1 & 0 \lt x \lt 1, 0 \lt y \lt 1 \\ 0 & \textrm{otherwise}\end{cases}
$$

Finding the PDF of $Z = X + Y$ is evaluating:

$$
\int_{0}^1 f_Y(y)f_X(z-y)dy
$$

Which can simplify (if we restrict $y$ to $[0,1]$) to:

$$
\int_{0}^1 f_X(z-y)dy
$$

Now, we can calculate the integral for the regions of $z$ which are valid for the range $y \in [0,1]$. Here, $f_X$ is one only if $0 \lt z - y \lt 1$. On the interval $0\lt z\lt 1$, the function is only one for $0\lt y\lt z$, so the integral simplifies to

$$
\int_0^z 1dy,\quad 0\lt z \lt 1
$$

Which is:

$$z$$

On the region $1\lt z \lt 2$, the function is only one for $z-1 \lt y \lt 1$, so the integral simplifies to

$$
\int_{z-1}^1 1dy,\quad 1\lt z \lt 2
$$

Which is:

$$2-z$$

So the PDF is:

$$
f_Z(z) = \begin{cases} z & 0 \lt z \lt 1 \\ 2-z & 1\lt z \lt 2  \end{cases}
$$

~~

## Expectation

Given some transformed RV $Z = g(X,Y)$, we can calculate the expectation of $Z$:

$$
\begin{gather*}
E[Z] = \int zf_Z(z)dz \\ 
\downarrow \\ 
E[g(x,y)] = \int g(x,y)f_{XY}(x,y)dxdy
\end{gather*}
$$

The same process can be done with discrete random variables, replacing the integral with a summation.

### Sum Expectation

$$
\begin{align*}
E[X+Y] &= \int_{-\infty}^\infty \int_{-\infty}^\infty (x+y)f(x,y)dxdy\\
  &= E[X] + E[Y]
\end{align*}
$$

This is true *regardless* of whether $X$ and $Y$ are independent.

The variance, on the other hand, ends up being:

$$
\begin{align*}
Var[X+Y] &= E[(X+Y-E[X+Y])^2] \\
  &= E[(X-E[X]+Y-E[Y])^2] \\
  &= E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])] \\
  &= Var[X] + Var[Y] + 2E[(X-E[X])(Y-E[Y])] \\
  &= Var[X] + Var[Y] + 2Cov(X,Y)
\end{align*}
$$

Where the last term, $E[ (X-E[X]) (Y-E[Y]) ]$ is called the **covariance** of $X$ and $Y$. A related value, the **Pearson Correlation**, is:

$$\rho (X,Y) = \frac{Cov(x,y)}{\sqrt{Var[X]}\sqrt{Var[Y]}}$$

If $X$ and $Y$ are independent, then the Covariance simplifies to:

$$Cov(X,Y) = E[X-E[X]]E[Y-E[Y]] = (E[X] - E[X])(E[Y] - E[Y]) = 0$$

This means that if $X$ and $Y$ are independent, the covariance is $0$, and the variance of the sum is the sum of the variance:

$$ Var[X + Y] = Var[X] + Var[Y] + 2Cov(X,Y) = Var[X] + Var[Y] $$

~~Example: circle expectation

From the previous examples, we have derived the PDF as:

$$f_D(r) = \frac{2r}{R^2},\quad 0 \le r \le R$$

Therefore, the expectation can be easily calculated as:

$$
E[D] = \int_0^R \frac{2r^2}{R^2}dr
$$

Which simplifies to:

$$ \frac{2}{R^2}\frac{R^3}{3} = \frac{2}{3}R $$

Alternatively, we can use the tranformation to calculate it:

$$
\begin{align*}
E[D] &= E[\sqrt{X^2 + Y^2}] \\
     &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \sqrt{x^2 + y^2}f(x,y)dxdy \\
     &= \frac{1}{\pi R^2} \int_0^{2\pi} \int_0^R r^2 dr d\theta \\
     &= \frac{2}{3}R
\end{align*}
$$

~~

### Product Expectation

*If $X$ and $Y$ are independent*:

$$E[XY] = E[X]E[Y]$$

This is true for any $g(x,y)$ which can be factored into two functions, $h(x)j(y)$:

$$E[g(x,y)] = E[h(X)]E[j(y)]$$

Again, *if the expectations are independent*.

<!-- ~~Example: Gaussian

Given $Z = X+Y$ and $X$ and $Y$ are independent Gaussian variables, find $F_Z$.

Because these are independent, we can use the moment generating function to create the expectation of the summation, showing it is just the product of the expectations of $X$ and $Y$. The same process can then be used to calculate the higher order moments and variance, which fully describes a Gaussian random variable.

~~ -->

~~Practice: true or false

$$ E\left[\frac{X}{Y}\right] = \frac{E[X]}{E[Y]} $$

This is **false**. This is a nonlinear function, so we cannot simplify using the linearity property of the expectation.

However, *if $X$ and $Y$ are independent*, then the following is true:

$$E[XY] = E[X]E[Y]$$

Which, if we change $Y$ to $\frac{1}{Y}$, we get:

$$ E\left[\frac{X}{Y}\right] E\left[X\frac{1}{Y}\right] = E[X]E\left[\frac{1}{Y}\right]$$

~~

<!-- ~~Practice: lambda

Given the number of customers in a restaurant is a Poisson RV with $\lambda = 100$. Each person that enters is a male with probability $0.6$ and female with probability $0.4$. $X$ is the number of males and $Y$ is the number of females.

a) are $X$ and $Y$ independent?

We need to prove $p_{XY}(x,y) = p_X(x)p_Y(y)$.

~~ -->

## Conditional Expectation

Conditional expectation is defined as:

$$E_{X|Y}[X|Y] = \int_{\Omega_x} xf_{x|y}(x|y)dx $$

This will be a random variable which is dependent on $y$, because it is conditioned on it.

### Adam's Law

The expectation of the conditional expectation:

$$E_Y[E_{X|Y}[X|Y]] = E_Y[g(Y)] = E_X[X]$$

This can be used to calculate the expectation of $X$ without directly calculating it, which is useful if it is very complex. It is similar to the divide-and-conquer approach which we used with total probability theorem.

<!-- This is related to **Eve's Law**:

$$Var[X] = E[Var[X|Y]] + Var$$ -->

<script src="/source/texme.js"></script>
<link rel="stylesheet" href="/source/theme.css">
