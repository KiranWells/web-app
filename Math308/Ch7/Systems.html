# Systems of Linear Equations - Introduction

A normal system of linear equations is similar to a normal system of equations:

$$
\begin{gathered}
  x' = f(x,y) \\
  y' = f(x,y)
\end{gathered}
$$

We can represent these geometrically as a velocity vector for a particle in the 2D plane. We can create a vector field for this, and then draw the path of that particle. If you would like to try this out, the program [pplane](https://www.cs.unm.edu/~joel/dfield/) can draw these.

This is especially useful for systems which are very difficult or impossible to solve.

Linear systems are linear in x and y:

$$
\begin{gathered}
  x' = a(t)x + b(t)y + f(t) \\
  y' = c(t)x + d(t)y + g(t) \\
\end{gathered}
$$

This is homogenous if $$f(t) = g(t) = 0$$. To solve these, we can use matrix form, representing the pair as a column matrix:

$$
\vec{x}(t) = 
\begin{bmatrix}
  x(t) \\
  y(t)
\end{bmatrix}
$$

So the differential can be represented as:

$$\vec{x}'(t) = A(t)\vec{x} + \vec{b}(t)$$

Where $A(t)$ is a matrix of the functions in front of x and y, and $\vec{b}(t)$ is the remaining functions.

$$
\begin{bmatrix}
  x' \\
  y'
\end{bmatrix}
=
\begin{bmatrix}
  a(t) && b(t) \\
  c(t) && d(t)
\end{bmatrix}
\begin{bmatrix}
  x \\
  y
\end{bmatrix}
+
\begin{bmatrix}
  f(t) \\
  g(t)
\end{bmatrix}
$$

### Theorem (Existence and Uniqueness Theorem)



In addition, all higher order ODEs in normal form can be converted to a system of ODEs, after indroducing two new variables $x_1$ and $x_2$.

$$y'' = F(t,y,y')$$

$$x_1 = y,\ \ \ x_2 = y'$$

So you get the system:

$$
\begin{gathered}
  x_1' = x_2 \\
  x_2' = y'' = F(t,x_1, x_2)
\end{gathered}
$$

Note that $x_2$ and the like are called the phase variables.

~~Example 2

Convert the IVP to a system:

$$y'' - 2ty' + t^3y = \cos(t), y(1)=3, y'(1) = -2$$

First, we convert it to normal form:

$$y'' = -t^3y + 2ty' + \cos(t)$$

Then, create the system:

$$
\begin{gathered}
  x_1' = x_2 \\
  x_2' = -t^3x_1 + 2tx_2 + \cos(t)
\end{gathered}
$$

And in matrix form:

$$
\begin{bmatrix}
  x_1' \\
  x_2'
\end{bmatrix}
=
\begin{bmatrix}
  0 && 1 \\
  -t^3 && 2t
\end{bmatrix}
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
+
\begin{bmatrix}
  0 \\
  \cos(t)
\end{bmatrix}
$$

~~

Systems can also be transformed into higher order functions by **elimination**. (Remember elimination is one way to solve systems by plugging in values from one equation to the other.)

~~Example 3

Solve the system:

$$
\begin{gathered}
  x_1' = x_1 - x_2 \\
  x_2' = -4x_1 + x_2
\end{gathered}
$$

First, we rarrange to try and eliminate $x_2$:

$$
\begin{gathered}
  x_2 = x_1 - x_1' \\
  x_2' = -4x_1 + x_2
\end{gathered}
$$

Then plug one value to another:

$$x_2 = x_1 - x_1'$$
$$(x_1 - x_1')' = -4x_1 +  x_1 - x_1'$$
$$x_1'' = 3x_1 + 2x_1'$$
$$x_1'' - 3x_1 - 2x_1' = 0$$

$$r = 3, -1$$

$$x_1 = c_1e^{3t} + c_2e^{-t}$$
$$x_2 = c_1e^{3t} + c_2e^{-t} - (3c_1e^{3t} - c_2e^{-t})$$
$$x_2 = -2c_1e^{3t} + 2c_2e^{-t}$$

Written in matrix form:

$$
\begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
=
\begin{bmatrix}
  c_1e^{3t} + c_2e^{-t} \\
  -2c_1e^{3t} + 2c_2e^{-t}
\end{bmatrix}
=
\begin{bmatrix}
  1 && 1 \\
  -2 && 2
\end{bmatrix}
\begin{bmatrix}
  c_1e^{3t} \\
  c_2e^{-t}
\end{bmatrix}
$$

But what we really want is a separation of the fundamental solutions:


$$
=
\begin{bmatrix}
  c_1e^{3t} \\
  -2c_1e^{3t}
\end{bmatrix}
+
\begin{bmatrix}
  c_2e^{-t} \\
  2c_2e^{-t}
\end{bmatrix}
=
c_1e^{3t}
\begin{bmatrix}
  1 \\
  -2
\end{bmatrix}
+
c_2e^{-t}
\begin{bmatrix}
  1 \\
  2
\end{bmatrix}
$$

Which gives what seems to be a normal fundamental pair of solutions, but in vector form.

Using one of the previous forms, we can say:

$$
\begin{bmatrix}
  1 && 1 \\
  -2 && 2
\end{bmatrix}
\begin{bmatrix}
  c_1e^{3t} \\
  c_2e^{-t}
\end{bmatrix}
=
\begin{bmatrix}
  e^{3t} && e^{-t} \\
  -2e^{3t} && 2e^{-t}
\end{bmatrix}
\begin{bmatrix}
  c_1 \\
  c_2
\end{bmatrix}
$$

Note that the columns of the second matrix contains the fundamental pairs of solutions we found earlier. This is called a fundemantal matrix, or $\Psi(t)$. This is the solution.

~~

This method is not always the preferred method, and the main way to solve these matrix equations, called the matrix method, is the topic of this chapter.

If, for some reason, we needed to go from a set of two vector solutions and find the homogeneous equation related to it, we can use the property that each vector alone will be a solution to the equation.

Given:

$$\vec x_1, \vec x_2$$

Find $P$:

$$\vec x' = P\vec x$$

So, we can either plug in $\vec x_1$ and $\vec x_2$ to the equation, as they are solutions:

$$\vec x_1' = P\vec x_1$$
$$\vec x_2' = P\vec x_2$$

Or, we can use the fundemental matrix $\Psi(t) = (\vec x_1 | \vec x_2)$. Note, we can only do this if the determinant of it, or the Wronskian of $x_1$ and $x_2$, are non-zero.

$$\Psi = (\vec x_1 | \vec x_2)$$
$$\Psi' = (\vec x_1' | \vec x_2')$$
$$\Psi' = (P(t)\vec x_1 | P(t)\vec x_2)$$
$$\Psi' = P(t)(\vec x_1 | \vec x_2)$$
$$\Psi' = P(t)\Psi$$

This shows that $\Psi$ is also a solution to the equation, and we can solve for $P(t)$ by multiplying by the inverse of $\Psi$ (which cancels it out on the right side):

$$\Psi' = P(t)\Psi$$
$$\Psi^{-1}\Psi' = P(t)\Psi\Psi^{-1}$$
$$\Psi^{-1}\Psi' = P(t)$$

Note that the inverse of $\Psi$, $\Psi^{-1}$ is:

$$
\Psi = 
\begin{bmatrix}
  a && b \\
  c && d
\end{bmatrix}
$$
$$
\Psi^{-1} = 
{1 \over ad - bc}
\begin{bmatrix}
  d && -b \\
  -c && a
\end{bmatrix}
$$

<script src="/source/texme.js"></script>
<link rel="stylesheet" href="/source/theme.css">
